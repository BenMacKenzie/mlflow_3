{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b5b281-89b3-4e47-8bd4-feae9d5697b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow openai databricks-agents\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd9b3ffa-e22f-4b47-8291-9bde95f6bb45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f8d27d0-e30e-44fe-9b29-fc98b3efe247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Nota Bene\n",
    "Bug in mlflow related to git folders. You must explicitly set the experiment if you want to create a labelling session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f365aba-579a-4158-a296-63e3cd027ae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_experiment(\"/Users/ben.mackenzie@databricks.com/agent_eval_chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c018ad04-9ef3-49c0-8f97-ec472369071f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from openai import OpenAI\n",
    "\n",
    "# Enable automatic tracing for all OpenAI API calls\n",
    "#mlflow.openai.autolog()\n",
    "\n",
    "# Connect to a Databricks LLM via OpenAI using the same credentials as MLflow\n",
    "# Alternatively, you can use your own OpenAI credentials here\n",
    "mlflow_creds = mlflow.utils.databricks_utils.get_databricks_host_creds()\n",
    "client = OpenAI(\n",
    "    api_key=mlflow_creds.token,\n",
    "    base_url=f\"{mlflow_creds.host}/serving-endpoints\"\n",
    ")\n",
    "\n",
    "# Create a RAG app with tracing\n",
    "@mlflow.trace\n",
    "def my_chatbot(user_question: str) -> str:\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_context(user_question)\n",
    "\n",
    "    # Generate response using LLM with retrieved context\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"databricks-llama-4-maverick\",  # If using OpenAI directly, use \"gpt-4o\" or \"gpt-3.5-turbo\"\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the provided context to answer questions.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {user_question}\"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "@mlflow.trace(span_type=\"RETRIEVER\")\n",
    "def retrieve_context(query: str) -> str:\n",
    "    # Simulated retrieval - in production, this would search a vector database\n",
    "    if \"mlflow\" in query.lower():\n",
    "        return \"MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for experiment tracking, model packaging, and deployment.\"\n",
    "    return \"General information about machine learning and data science.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033a6609-56a7-4c0c-bd74-d649c68e097c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the app to generate a trace\n",
    "response = my_chatbot(\"What is MLflow?\")\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Get the trace ID for the next step\n",
    "trace_id = mlflow.get_last_active_trace_id()\n",
    "print(f\"Trace ID: {trace_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2180e8fc-669c-4917-8af0-5df8ae9c9221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.entities.assessment import AssessmentSource, AssessmentSourceType\n",
    "\n",
    "# Simulate end-user feedback from your app\n",
    "# In production, this would be triggered when a user clicks thumbs down in your UI\n",
    "mlflow.log_feedback(\n",
    "    trace_id=trace_id,\n",
    "    name=\"user_feedback\",\n",
    "    value=False,  # False for thumbs down - user is unsatisfied\n",
    "    rationale=\"Missing details about MLflow's key features like model evaluation\",\n",
    "    source=AssessmentSource(\n",
    "        source_type=AssessmentSourceType.HUMAN,\n",
    "        source_id=\"enduser_123\",  # Would be actual user ID in production\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"✅ End-user feedback recorded!\")\n",
    "\n",
    "# In a real app, you would:\n",
    "# 1. Return the trace_id with your response to the frontend\n",
    "# 2. When user clicks thumbs up/down, call your backend API\n",
    "# 3. Your backend would then call mlflow.log_feedback() with the trace_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e21e8dd7-ca4d-40f9-a822-ea8ba55edd48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Nota Bene\n",
    "End user feeback is an assessment (could be used by a custom Scorer but not used by e.g., Correctness scorer which requires expected facts or expected response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f80618-16da-44c3-b704-9722778d8cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.label_schemas import create_label_schema, InputCategorical, InputText, InputTextList\n",
    "\n",
    "\n",
    "# Define what feedback to collect\n",
    "accuracy_schema = create_label_schema(\n",
    "    name=\"response_accuracy\",\n",
    "    type=\"feedback\",\n",
    "    title=\"Is the response factually accurate?\",\n",
    "    input=InputCategorical(options=[\"Accurate\", \"Partially Accurate\", \"Inaccurate\"]),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "expected_facts_schema = create_label_schema(\n",
    "    name=\"expected_facts\",\n",
    "    type=\"expectation\",\n",
    "    title=\"What are the expected facts?\",\n",
    "    input=InputTextList(),\n",
    "    overwrite=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03735e23-3e9a-4bc6-bcaf-0e3db44caa33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.labeling import create_labeling_session\n",
    "\n",
    "# Create a labeling session\n",
    "labeling_session = create_labeling_session(\n",
    "    name=\"review_with_expected_facts_2\",\n",
    "    label_schemas=[\"expected_facts\", \"response_accuracy\"],\n",
    ")\n",
    "# Add your trace to the session\n",
    "# Get the most recent trace from the current experiment\n",
    "traces = mlflow.search_traces(\n",
    "    max_results=1  # Gets the most recent trace\n",
    ")\n",
    "labeling_session.add_traces(traces)\n",
    "\n",
    "# Share with reviewers\n",
    "print(f\"✅ Trace sent for review!\")\n",
    "print(f\"Share this link with reviewers: {labeling_session.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89a279e-412f-48bf-983f-da7d77b6006f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labeled_traces = mlflow.search_traces(\n",
    "    run_id=labeling_session.mlflow_run_id,  # Labeling Sessions are MLflow Runs\n",
    ")\n",
    "\n",
    "# or \n",
    "# labeled_traces = mlflow.search_traces(run_id=\"5387ba7c2a284e0d92ebf5e3a2c6b1ab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8360e264-8ad9-4cd9-a92a-73b3fb688a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Have a look at trace in experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7195dc-e19f-4a17-8b31-08156a78c52b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.genai.datasets\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. Create an evaluation dataset\n",
    "\n",
    "evaluation_dataset_table_name = \"human_feedback_eval_2\"\n",
    "\n",
    "UC_TABLE_NAME = f\"{eval_schema}.{evaluation_dataset_table_name}\"\n",
    "\n",
    "eval_dataset = mlflow.genai.datasets.create_dataset(\n",
    "    uc_table_name=UC_TABLE_NAME,\n",
    ")\n",
    "\n",
    "#eval_dataset = mlflow.genai.datasets.get_dataset(UC_TABLE_NAME)\n",
    "print(f\"Created evaluation dataset: UC_TABLE_NAME\")\n",
    "\n",
    "eval_dataset = eval_dataset.merge_records(labeled_traces)\n",
    "print(f\"Added {len(traces)} records to evaluation dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6580ed59-4b8a-4b26-8be1-96d1cefe3fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Nota Bene\n",
    "We are creating an eval data set using traces.  We could have created with a list...but we would have had to add expectations to run Correctness Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32d5178-d14a-48cc-a550-0fbf305bc0c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import Correctness\n",
    "\n",
    "# Evaluate your app against expert expectations\n",
    "with mlflow.start_run(run_name=\"v1\"):\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=eval_dataset,\n",
    "        predict_fn=my_chatbot,  # The app we created in Step 1\n",
    "        scorers=[Correctness()]  # Compares outputs to expected_response\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20413a31-6ddd-4970-b34e-da997be54a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Nota Bene\n",
    "\n",
    "Running eval on a dataset created from traces creates new traces. The new trace does not contain the original human assesemment but does include the expectations created in labelling session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7179f42-9b1b-4abe-8776-ff9619dd5d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets try to create a labelling session through the UI.  First lets' create a new trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0353f106-ac53-44da-b319-6ef87528c975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = my_chatbot(\"What is the difference between databricks MLflow? and open source mlflow?\")\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Get the trace ID for the next step\n",
    "trace_id = mlflow.get_last_active_trace_id()\n",
    "print(f\"Trace ID: {trace_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35c12e9a-6565-455c-87a6-c43bbc03c6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now add the new trace with assessment to the evaluation dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5b54527-2724-4f5c-8438-3cbab795a179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Nota Bene\n",
    "This can be done through UI..but there is a bug the create multiple entries in selection box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee9e54b-a6f4-4991-939e-8f89eb92a75e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#not sure how how to find a trace based on the id...\n",
    "\n",
    "traces = mlflow.search_traces(\n",
    "    filter_string=\"tags.demo = '1'\"\n",
    ")\n",
    "display(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42fee5c5-47ba-4bfa-a25f-f14cc57db276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.merge_records(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a977a935-c330-442b-8f58-d4f0444bb682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Go to UI to create Labelling Session.  Then come back and run evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eda51c8-d2b9-49c1-a200-537f84993cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import Correctness\n",
    "\n",
    "# Evaluate your app against expert expectations\n",
    "with mlflow.start_run(run_name=\"v4\"):\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=eval_dataset,\n",
    "        predict_fn=my_chatbot,  # The app we created in Step 1\n",
    "        scorers=[Correctness()]  # Compares outputs to expected_response\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4640557381875100,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "3_Evaluate_Chatbot_Human_Feedback",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
