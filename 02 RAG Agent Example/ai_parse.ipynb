{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ca0a52c-2485-463e-9991-cb3c3b195d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Document Service\n",
    "\n",
    "This notebook forms the core of our document service. It showcases how we are going to simplify our document intelligence application using Lakebase and Serverless jobs. This is tested on Serverless Version 3 - it takes a single file or a directory and parses all the files directly into an append operation on a postgres table. We can then get embeddings and use pgvector as the backend with a langgraph Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa2dcd5f-f302-4b73-a6d2-1cd8cdaedf1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We use our Databricks user IDs as the main entry point into the workflow and authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe65912b-8c74-44d8-a0d6-bf4cff33d3b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-langchain databricks-sdk --upgrade\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e367d59-8f14-4de5-8b45-ba36d835fda7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "me = w.current_user.me()\n",
    "print(me.id)  # This is your Databricks user ID\n",
    "print(me.user_name) \n",
    "USER_ID = me.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa248897-5edc-46ba-a79e-4aee631c1a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We use ai_parse_document in a serverless job as our document processing service. This could be any isolated microservice and has lots of room for optimization, but ai_parse_document does a pretty good job and can handle lots of file types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1abdb113-df99-481a-a6e7-98f59c4b9199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_df = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .load(config.get(\"file_path\"))\n",
    "    .withColumn(\"user_id\", lit(USER_ID))\n",
    "    .select(\n",
    "        col(\"path\"),\n",
    "        col(\"user_id\"),\n",
    "        expr(\"ai_parse_document(content)\").alias(\"parsed\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"parsed_json\",\n",
    "        parse_json(col(\"parsed\").cast(\"string\"))\n",
    "    )\n",
    "    .select(\n",
    "        col(\"path\"),\n",
    "        col(\"user_id\"),\n",
    "        expr(\"parsed_json:document:pages\").alias(\"pages\"),\n",
    "        expr(\"parsed_json:document:elements\").alias(\"elements\"),\n",
    "        expr(\"parsed_json:document:_corrupted_data\").alias(\"_corrupted_data\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1744a57-2c1f-43c7-90e9-113541b536b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To get something simple and working, I propose that we simply chunk each page for now. We can work on refining the chunking strategy in this job, but this gives a good starting point. We even wrap the embedding call here for better horizontal scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e80b7b8-2dfd-4aca-bf64-13e9890bc2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, explode, col, concat_ws, lit, expr\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, IntegerType, StringType\n",
    "import uuid\n",
    "\n",
    "# Define schema for pages based on provided example\n",
    "page_schema = StructType([\n",
    "    StructField(\"content\", StringType()),\n",
    "    StructField(\"footer\", StringType()),\n",
    "    StructField(\"header\", StringType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"page_number\", IntegerType())\n",
    "])\n",
    "\n",
    "chunked_pages = (\n",
    "    parsed_df\n",
    "    .withColumn(\n",
    "        \"pages_array\",\n",
    "        from_json(\n",
    "            col(\"pages\").cast(\"string\"),\n",
    "            ArrayType(page_schema)\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"page_chunk\",\n",
    "        explode(col(\"pages_array\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"doc_id\",\n",
    "        lit(config['doc_id'])\n",
    "    )\n",
    "    .select(\n",
    "        lit(str(uuid.uuid4())).alias(\"id\"),\n",
    "        col(\"doc_id\"),\n",
    "        array(col(\"page_chunk.id\").cast(\"string\")).alias(\"page_ids\"),\n",
    "        concat_ws(\n",
    "            \"\\n\",\n",
    "            concat_ws(\"\", lit(\"Content: [\"), col(\"page_chunk.content\"), lit(\"]\")),\n",
    "            concat_ws(\"\", lit(\"Footer: [\"), col(\"page_chunk.footer\"), lit(\"]\")),\n",
    "            concat_ws(\"\", lit(\"Header: [\"), col(\"page_chunk.header\"), lit(\"]\")),\n",
    "            concat_ws(\"\", lit(\"ID: [\"), col(\"page_chunk.id\").cast(\"string\"), lit(\"]\")),\n",
    "            concat_ws(\"\", lit(\"Page Number: [\"), col(\"page_chunk.page_number\").cast(\"string\"), lit(\"]\"))\n",
    "        ).alias(\"content\")\n",
    "    )\n",
    "    .withColumn(\"embedding\", expr(f\"ai_query('{config.get('embedding_endpoint')}', content)\"))\n",
    "    .withColumn(\"metadata\", to_json(struct(col(\"_metadata\"))))\n",
    "    .withColumn(\"created_at\", current_timestamp())\n",
    ")\n",
    "\n",
    "display(chunked_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e89699-851d-45a2-9423-6246eb415491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunked_pages_pd = chunked_pages.toPandas()\n",
    "chunked_pages_pd['embedding'] = chunked_pages_pd['embedding'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67037be3-f740-43ad-a925-b2e418c55422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunked_pages_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93c45e41-7569-4999-8a4f-c17cfd81c2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Postgres Connection\n",
    "Now we are going to move our chunks into postgres. First we create our chunks table, next we read into Pandas and write chunks into Postgres. We can bolster the connection and database for horizontal scalability (e.g. https://learn.microsoft.com/en-us/azure/databricks/oltp/query/notebook).\n",
    "\n",
    "This notebook assumes a working database instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0d3a0b-99b3-463c-b9d5-e66eacdd73a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.database import DatabaseInstance\n",
    "\n",
    "import psycopg2\n",
    "import uuid\n",
    "\n",
    "CRED = w.database.generate_database_credential(request_id=str(uuid.uuid4()), instance_names=[instance_name])\n",
    "\n",
    "def connect_to_pg():\n",
    "  conn = psycopg2.connect(\n",
    "      host=instance.read_write_dns,\n",
    "      dbname=\"databricks_postgres\",\n",
    "      user=me.user_name,\n",
    "      password=CRED.token,\n",
    "      sslmode=\"require\"\n",
    "  )\n",
    "  return conn\n",
    "\n",
    "def run_pg_query(query, data_tuples=None):\n",
    "  conn = connect_to_pg()\n",
    "  with conn.cursor() as cur:\n",
    "      if data_tuples:\n",
    "        cur.executemany(query, data_tuples)\n",
    "      else:\n",
    "        cur.execute(query)\n",
    "      conn.commit()\n",
    "  conn.close()\n",
    "  return True\n",
    "\n",
    "w = WorkspaceClient()\n",
    "instance = w.database.get_database_instance(name=config.get(\"database_instance\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "011f3977-6445-4166-b171-558a011f98ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Make our table if it doesn't exist. This takes <10 μs so isn't a huge production risk to run with every job. Note the unique constraint to avoid duplication of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c05f6fa-be07-4b65-a8eb-ec8ab34dd410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%time\n",
    "run_pg_query(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "\n",
    "run_pg_query(\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS parsed_pages (\n",
    "        path TEXT,\n",
    "        user_id TEXT,\n",
    "        page_id TEXT,\n",
    "        text TEXT,\n",
    "        embedding VECTOR(1024),\n",
    "        CONSTRAINT user_path_page_pk PRIMARY KEY (user_id, path, page_id)\n",
    "    );\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f59133d-9d68-4f08-b1b2-6a0146e210b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Insert records into our PG table. We keep appending the table (could have individual user tables but this is unnecessary in my opinion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62c71b5-a5f4-477c-9631-5a04631355ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_tuples = [tuple(x) for x in chunked_pages_pd.to_numpy()]\n",
    "insert_query = \"\"\"\n",
    "  INSERT INTO parsed_pages (path, user_id, page_id, text, embedding) \n",
    "  VALUES (%s, %s, %s, %s, %s)\n",
    "  ON CONFLICT (path, user_id, page_id) DO NOTHING\n",
    "  \"\"\"\n",
    "run_pg_query(insert_query, data_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "815dc32d-5147-4af9-9739-0491d8949ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Test the table insert went through by pulling the first row. We now have a postgres table with embeddings ready to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695e83a4-6c33-463c-9910-9bab08cf19c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "conn = connect_to_pg()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT * FROM parsed_pages\")\n",
    "    print(cur.fetchone())\n",
    "    conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ai_parse",
   "widgets": {
    "database_instance": {
     "currentValue": "shm",
     "nuid": "c10c5b93-fb6d-47af-ab2f-91f0d0a73bc2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "shm",
      "label": null,
      "name": "database_instance",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "shm",
      "label": null,
      "name": "database_instance",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "doc_id": {
     "currentValue": "3450234095",
     "nuid": "b9abcd48-04d8-469b-b20a-8995b828c841",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "doc_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "doc_id",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "embedding_endpoint": {
     "currentValue": "databricks-gte-large-en",
     "nuid": "65d5b79f-7a86-4338-ad2a-48958e6e157b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_endpoint",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_endpoint",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "file_path": {
     "currentValue": "/Volumes/shm/doc_intel/raw_pdfs/test.pdf",
     "nuid": "143d3171-7ba0-4d14-a537-59aaf785d3ba",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/main/default/raw_pdfs/test.pdf",
      "label": null,
      "name": "file_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/main/default/raw_pdfs/test.pdf",
      "label": null,
      "name": "file_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
