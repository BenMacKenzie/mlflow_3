{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b5b281-89b3-4e47-8bd4-feae9d5697b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade \"mlflow[databricks]>=3.1.0\" openai \"databricks-connect>=16.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c018ad04-9ef3-49c0-8f97-ec472369071f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from openai import OpenAI\n",
    "\n",
    "# Enable automatic tracing for all OpenAI API calls\n",
    "#mlflow.openai.autolog()\n",
    "\n",
    "# Connect to a Databricks LLM via OpenAI using the same credentials as MLflow\n",
    "# Alternatively, you can use your own OpenAI credentials here\n",
    "mlflow_creds = mlflow.utils.databricks_utils.get_databricks_host_creds()\n",
    "client = OpenAI(\n",
    "    api_key=mlflow_creds.token,\n",
    "    base_url=f\"{mlflow_creds.host}/serving-endpoints\"\n",
    ")\n",
    "\n",
    "# Create a RAG app with tracing\n",
    "@mlflow.trace\n",
    "def my_chatbot(user_question: str) -> str:\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_context(user_question)\n",
    "\n",
    "    # Generate response using LLM with retrieved context\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"databricks-llama-4-maverick\",  # If using OpenAI directly, use \"gpt-4o\" or \"gpt-3.5-turbo\"\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the provided context to answer questions.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {user_question}\"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "@mlflow.trace(span_type=\"RETRIEVER\")\n",
    "def retrieve_context(query: str) -> str:\n",
    "    # Simulated retrieval - in production, this would search a vector database\n",
    "    if \"mlflow\" in query.lower():\n",
    "        return \"MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for experiment tracking, model packaging, and deployment.\"\n",
    "    return \"General information about machine learning and data science.\"\n",
    "\n",
    "# Run the app to generate a trace\n",
    "response = my_chatbot(\"What is MLflow?\")\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Get the trace ID for the next step\n",
    "trace_id = mlflow.get_last_active_trace_id()\n",
    "print(f\"Trace ID: {trace_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2180e8fc-669c-4917-8af0-5df8ae9c9221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.entities.assessment import AssessmentSource, AssessmentSourceType\n",
    "\n",
    "# Simulate end-user feedback from your app\n",
    "# In production, this would be triggered when a user clicks thumbs down in your UI\n",
    "mlflow.log_feedback(\n",
    "    trace_id=trace_id,\n",
    "    name=\"user_feedback\",\n",
    "    value=False,  # False for thumbs down - user is unsatisfied\n",
    "    rationale=\"Missing details about MLflow's key features like Projects and Model Registry\",\n",
    "    source=AssessmentSource(\n",
    "        source_type=AssessmentSourceType.HUMAN,\n",
    "        source_id=\"enduser_123\",  # Would be actual user ID in production\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"✅ End-user feedback recorded!\")\n",
    "\n",
    "# In a real app, you would:\n",
    "# 1. Return the trace_id with your response to the frontend\n",
    "# 2. When user clicks thumbs up/down, call your backend API\n",
    "# 3. Your backend would then call mlflow.log_feedback() with the trace_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f80618-16da-44c3-b704-9722778d8cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.label_schemas import create_label_schema, InputCategorical, InputText, InputTextList\n",
    "from mlflow.genai.labeling import create_labeling_session\n",
    "\n",
    "# Define what feedback to collect\n",
    "expected_facts_schema = create_label_schema(\n",
    "    name=\"expected_facts\",\n",
    "    type=\"expectation\",  #Correctness judge requires expectations type\n",
    "    title=\"What are the expected facts?\",\n",
    "    input=InputTextList(),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "\n",
    "# Create a labeling session\n",
    "labeling_session = create_labeling_session(\n",
    "    name=\"quickstart_review\",\n",
    "    label_schemas=[expected_facts_schema.name],\n",
    ")\n",
    "\n",
    "# Add your trace to the session\n",
    "# Get the most recent trace from the current experiment\n",
    "traces = mlflow.search_traces(\n",
    "    max_results=1  # Gets the most recent trace\n",
    ")\n",
    "labeling_session.add_traces(traces)\n",
    "\n",
    "# Share with reviewers\n",
    "print(f\"✅ Trace sent for review!\")\n",
    "print(f\"Share this link with reviewers: {labeling_session.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89a279e-412f-48bf-983f-da7d77b6006f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labeled_traces = mlflow.search_traces(\n",
    "    run_id=labeling_session.mlflow_run_id,  # Labeling Sessions are MLflow Runs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7603596c-4197-4f64-a8a8-f983619b9b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labeled_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7195dc-e19f-4a17-8b31-08156a78c52b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.genai.datasets\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. Create an evaluation dataset\n",
    "\n",
    "# Replace with a  Catalog schema where you have CREATE TABLE permission\n",
    "uc_schema = \"ml_demo.default\"\n",
    "# This table will be created in the above UC schema\n",
    "evaluation_dataset_table_name = \"databricks_agent_eval_v2\"\n",
    "\n",
    "mlflow.genai.delete_dataset(uc_table_name=f\"{uc_schema}.{evaluation_dataset_table_name}\")\n",
    "\n",
    "eval_dataset = mlflow.genai.datasets.create_dataset(\n",
    "    uc_table_name=f\"{uc_schema}.{evaluation_dataset_table_name}\",\n",
    ")\n",
    "print(f\"Created evaluation dataset: {uc_schema}.{evaluation_dataset_table_name}\")\n",
    "\n",
    "\n",
    "eval_dataset.merge_records(labeled_traces)\n",
    "print(f\"Added {len(traces)} records to evaluation dataset\")\n",
    "\n",
    "# Preview the dataset\n",
    "df = eval_dataset.to_df()\n",
    "print(f\"\\nDataset preview:\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(\"\\nSample record:\")\n",
    "sample = df.iloc[0]\n",
    "print(f\"Inputs: {sample['inputs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5cd7da4-1b43-41b5-b0c8-0b371367bd88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32d5178-d14a-48cc-a550-0fbf305bc0c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import Correctness\n",
    "\n",
    "# Evaluate your app against expert expectations\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset.to_df(),\n",
    "    predict_fn=my_chatbot,  # The app we created in Step 1\n",
    "    scorers=[Correctness()]  # Compares outputs to expected_response\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_Human_Feedback_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
