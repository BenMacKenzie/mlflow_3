{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b4765e-cf9d-4b41-a9bd-7058eaa87f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade \"mlflow[databricks]>=3.1.0\" openai \"databricks-connect>=16.1\" \"databricks-langchain>=0.3.0\"\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc317cac-413c-49f9-a731-0b59bc42b5e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reference:\n",
    "https://learn.microsoft.com/en-ca/azure/databricks/mlflow3/genai/tracing/app-instrumentation/automatic\n",
    "\n",
    "Notice that we get the trace without an associated run (navigate to experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c86973-2e2c-4afb-aac1-561fd5824e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from openai import OpenAI\n",
    "\n",
    "# Enable MLflow's autologging to instrument your application with Tracing\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Create an OpenAI client that is connected to Databricks hosted LLMs\n",
    "mlflow_creds = mlflow.utils.databricks_utils.get_databricks_host_creds()\n",
    "client = OpenAI(\n",
    "    api_key=mlflow_creds.token,\n",
    "    base_url=f\"{mlflow_creds.host}/serving-endpoints\"\n",
    ")\n",
    "\n",
    "# Use the trace decorator to capture the application's entry point\n",
    "@mlflow.trace\n",
    "def my_app(input: str):\n",
    "  # This call is automatically instrumented by `mlflow.openai.autolog()`\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"databricks-meta-llama-3-1-8b-instruct\",  # This example uses a Databricks hosted LLM - you can replace this with any AI Gateway or Model Serving endpoint. If you have an external model endpoint configured, you can use that model name here.\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\",\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": input,\n",
    "      },\n",
    "    ],\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "result = my_app(input=\"What is MLflow?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7942c7ce-4a53-4334-86cc-aa63844752a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Note that you are not required to use open ai client\n",
    "\n",
    "and notice that the trace is stored with experiment without differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bff4a758-6516-42cb-9b64-4dd043e45e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "import mlflow\n",
    "\n",
    "\n",
    "chat_model = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-1-8b-instruct\"\n",
    ")\n",
    "\n",
    "@mlflow.trace\n",
    "def get_chat_response(messages):\n",
    "  result = chat_model.invoke(messages)\n",
    "  return result\n",
    "\n",
    "messages=[\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\",\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"what is mlflow\",\n",
    "      },\n",
    "    ]\n",
    "  \n",
    "\n",
    "#result = chat_model.invoke(messages)\n",
    "result = get_chat_response(messages)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43771a0e-89fb-4674-ad5b-41861c2a72ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0_Instrument_App",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
