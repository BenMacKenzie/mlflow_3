{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b5b281-89b3-4e47-8bd4-feae9d5697b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade \"mlflow[databricks]>=3.1.0\" openai \"databricks-connect>=16.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd9b3ffa-e22f-4b47-8291-9bde95f6bb45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f365aba-579a-4158-a296-63e3cd027ae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#you have to set this explicilty.  Bug in mlflow.  You will get a permission error when setting up human feedback. \n",
    "\n",
    "mlflow.set_experiment(\"/Users/benmackenzie3775@gmail.com/2_Human_Feedback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60452725-146d-4a16-ae62-46a3b384fe33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c018ad04-9ef3-49c0-8f97-ec472369071f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from openai import OpenAI\n",
    "\n",
    "# Enable automatic tracing for all OpenAI API calls\n",
    "#mlflow.openai.autolog()\n",
    "\n",
    "# Connect to a Databricks LLM via OpenAI using the same credentials as MLflow\n",
    "# Alternatively, you can use your own OpenAI credentials here\n",
    "mlflow_creds = mlflow.utils.databricks_utils.get_databricks_host_creds()\n",
    "client = OpenAI(\n",
    "    api_key=mlflow_creds.token,\n",
    "    base_url=f\"{mlflow_creds.host}/serving-endpoints\"\n",
    ")\n",
    "\n",
    "# Create a RAG app with tracing\n",
    "@mlflow.trace\n",
    "def my_chatbot(user_question: str) -> str:\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_context(user_question)\n",
    "\n",
    "    # Generate response using LLM with retrieved context\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"databricks-llama-4-maverick\",  # If using OpenAI directly, use \"gpt-4o\" or \"gpt-3.5-turbo\"\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the provided context to answer questions.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {user_question}\"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "@mlflow.trace(span_type=\"RETRIEVER\")\n",
    "def retrieve_context(query: str) -> str:\n",
    "    # Simulated retrieval - in production, this would search a vector database\n",
    "    if \"mlflow\" in query.lower():\n",
    "        return \"MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for experiment tracking, model packaging, and deployment.\"\n",
    "    return \"General information about machine learning and data science.\"\n",
    "\n",
    "# Run the app to generate a trace\n",
    "response = my_chatbot(\"What is MLflow?\")\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Get the trace ID for the next step\n",
    "trace_id = mlflow.get_last_active_trace_id()\n",
    "print(f\"Trace ID: {trace_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2180e8fc-669c-4917-8af0-5df8ae9c9221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.entities.assessment import AssessmentSource, AssessmentSourceType\n",
    "\n",
    "# Simulate end-user feedback from your app\n",
    "# In production, this would be triggered when a user clicks thumbs down in your UI\n",
    "mlflow.log_feedback(\n",
    "    trace_id=trace_id,\n",
    "    name=\"user_feedback\",\n",
    "    value=False,  # False for thumbs down - user is unsatisfied\n",
    "    rationale=\"Missing details about MLflow's key features like Projects and Model Registry\",\n",
    "    source=AssessmentSource(\n",
    "        source_type=AssessmentSourceType.HUMAN,\n",
    "        source_id=\"enduser_123\",  # Would be actual user ID in production\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"✅ End-user feedback recorded!\")\n",
    "\n",
    "# In a real app, you would:\n",
    "# 1. Return the trace_id with your response to the frontend\n",
    "# 2. When user clicks thumbs up/down, call your backend API\n",
    "# 3. Your backend would then call mlflow.log_feedback() with the trace_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f80618-16da-44c3-b704-9722778d8cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.label_schemas import create_label_schema, InputCategorical, InputText\n",
    "from mlflow.genai.labeling import create_labeling_session\n",
    "\n",
    "# Define what feedback to collect\n",
    "accuracy_schema = create_label_schema(\n",
    "    name=\"response_accuracy\",\n",
    "    type=\"feedback\",\n",
    "    title=\"Is the response factually accurate?\",\n",
    "    input=InputCategorical(options=[\"Accurate\", \"Partially Accurate\", \"Inaccurate\"]),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "ideal_response_schema = create_label_schema(\n",
    "    name=\"expected_response\",\n",
    "    type=\"expectation\",\n",
    "    title=\"What would be the ideal response?\",\n",
    "    input=InputText(),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Create a labeling session\n",
    "labeling_session = create_labeling_session(\n",
    "    name=\"quickstart_review\",\n",
    "    label_schemas=[accuracy_schema.name, ideal_response_schema.name],\n",
    ")\n",
    "\n",
    "# Add your trace to the session\n",
    "# Get the most recent trace from the current experiment\n",
    "traces = mlflow.search_traces(\n",
    "    max_results=1  # Gets the most recent trace\n",
    ")\n",
    "labeling_session.add_traces(traces)\n",
    "\n",
    "# Share with reviewers\n",
    "print(f\"✅ Trace sent for review!\")\n",
    "print(f\"Share this link with reviewers: {labeling_session.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89a279e-412f-48bf-983f-da7d77b6006f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labeled_traces = mlflow.search_traces(\n",
    "    run_id=labeling_session.mlflow_run_id,  # Labeling Sessions are MLflow Runs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7195dc-e19f-4a17-8b31-08156a78c52b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.genai.datasets\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. Create an evaluation dataset\n",
    "\n",
    "evaluation_dataset_table_name = \"human_feedback_eval\"\n",
    "\n",
    "UC_TABLE_NAME = f\"{quickstart_schema}.{evaluation_dataset_table_name}\"\n",
    "\n",
    "eval_dataset = mlflow.genai.datasets.create_dataset(\n",
    "    uc_table_name=UC_TABLE_NAME,\n",
    ")\n",
    "print(f\"Created evaluation dataset: UC_TABLE_NAME\")\n",
    "\n",
    "eval_dataset.merge_records(labeled_traces)\n",
    "print(f\"Added {len(traces)} records to evaluation dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d57a2de-e07c-4654-b886-4beeb81115d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#bug in mlflow requires you to get the dataset again.  should be resolved soon.  https://databricks.slack.com/archives/C083A8HQC6N/p1752510950155989?thread_ts=1751374306.654719&cid=C083A8HQC6N\n",
    "\n",
    "eval_dataset = mlflow.genai.datasets.get_dataset(UC_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32d5178-d14a-48cc-a550-0fbf305bc0c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import Correctness\n",
    "\n",
    "# Evaluate your app against expert expectations\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=my_chatbot,  # The app we created in Step 1\n",
    "    scorers=[Correctness()]  # Compares outputs to expected_response\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_Human_Feedback",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
