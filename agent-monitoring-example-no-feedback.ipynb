{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17990ddd-3854-4f07-b427-70073a7c617b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Use Lakehouse Monitoring for GenAI to monitor your production agent\n",
    "\n",
    "This notebook demonstrates how to monitor a deployed GenAI app / Agent using Lakehouse Monitoring for GenAI. It will:\n",
    "1. Deploy a \"hello world\" agent using Agent Framework.\n",
    "2. Configure quality monitoring using Agent Evaluation's LLM judges.\n",
    "3. Send sample traffic to the deployed endpoint.\n",
    "\n",
    "Lakehouse Monitoring for GenAI allows you to:\n",
    "- Track quality and operational performance (latency, request volume, errors, etc.).\n",
    "- Run LLM-based evaluations on production traffic to detect drift or regressions using Agent Evaluation's [LLM judges](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/llm-judge-reference)\n",
    "- Deep dive into individual requests to debug and improve agent responses.\n",
    "- Transform real-world logs into evaluation sets to drive continuous improvements.\n",
    "\n",
    "**Note:** When you deploy agents authored with [ChatAgent](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent#-use-chatagent-to-author-agents) using Agent Frameworks' `agents.deploy(...)`, basic monitoring is automatically configured with operational metrics (request volume, latency, error rate, etc).  You can optionally configure quality metrics using Agent Evaluation's [propietary LLM judges](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/llm-judge-reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5a16e8-6a59-4543-ac73-7cc0fc66ea72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install dependencies"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq databricks-agents>=0.17.0 databricks-sdk[openai] backoff uv\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10a6050d-79d8-454f-94e7-0ce504cb882b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Select a Unity Catalog schema\n",
    "\n",
    "Ensure you have CREATE TABLE and CREATE MODEL access in this schema.  By default, these values are set to your workspace's default catalog & schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00abb0da-bc85-4b84-9dee-301de7307ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the workspace default UC catalog / schema\n",
    "uc_default_location = spark.sql(\"select current_catalog() as current_catalog, current_schema() as current_schema\").collect()[0]\n",
    "current_catalog = uc_default_location[\"current_catalog\"]\n",
    "current_schema = uc_default_location[\"current_schema\"]\n",
    "\n",
    "\n",
    "# Modify the UC catalog / schema here or at the top of the notebook in the widget editor\n",
    "dbutils.widgets.text(\"uc_catalog\", current_catalog)\n",
    "dbutils.widgets.text(\"uc_schema\", current_schema)\n",
    "UC_CATALOG = dbutils.widgets.get(\"uc_catalog\")\n",
    "UC_SCHEMA = dbutils.widgets.get(\"uc_schema\")\n",
    "UC_PREFIX = f\"{UC_CATALOG}.{UC_SCHEMA}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6e36da2-1041-4579-a740-c5ac38f540a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agent creation and deployment\n",
    "\n",
    "In this section, we will:\n",
    "1. Create a simple agent by using Llama 70B\n",
    "2. Log the agent using MLflow\n",
    "3. Deploy the agent. This will automatically setup basic monitoring that tracks request volume, latency, and errors.\n",
    "\n",
    "You can skip this step if you already have a deployed agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8715d325-f9d1-4e82-a7f8-dee469e5bd08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile hello_world_agent.py\n",
    "from typing import Any, Generator, Optional\n",
    "\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.pyfunc.model import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Optional: Replace with any model serving endpoint\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "\n",
    "class SimpleChatAgent(ChatAgent):\n",
    "    def __init__(self):\n",
    "        self.workspace_client = WorkspaceClient()\n",
    "        self.client = self.workspace_client.serving_endpoints.get_open_ai_client()\n",
    "        self.llm_endpoint = LLM_ENDPOINT_NAME\n",
    "\n",
    "        # Fake documents to simulate the retriever\n",
    "        self.documents = [\n",
    "            mlflow.entities.Document(\n",
    "                metadata={\"doc_uri\": \"uri1.txt\"},\n",
    "                page_content=\"\"\"Lakehouse Monitoring for GenAI helps you monitor the quality, cost, and latency of production GenAI apps.  Lakehouse Monitoring for GenAI allows you to:\\n- Track quality and operational performance (latency, request volume, errors, etc.).\\n- Run LLM-based evaluations on production traffic to detect drift or regressions using Agent Evaluation's LLM judges.\\n- Deep dive into individual requests to debug and improve agent responses.\\n- Transform real-world logs into evaluation sets to drive continuous improvements.\"\"\",\n",
    "            ),\n",
    "            # This is a new document about spark.\n",
    "            mlflow.entities.Document(\n",
    "                metadata={\"doc_uri\": \"uri2.txt\"},\n",
    "                page_content=\"The latest spark version in databricks in 3.5.0\",\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # Tell Agent Evaluation's judges and review app about the schema of your retrieved documents\n",
    "        mlflow.models.set_retriever_schema(\n",
    "            name=\"fake_vector_search\",\n",
    "            primary_key=\"doc_uri\",\n",
    "            text_column=\"page_content\",\n",
    "            doc_uri=\"doc_uri\"\n",
    "            # other_columns=[\"column1\", \"column2\"],\n",
    "        )\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.RETRIEVER)\n",
    "    def dummy_retriever(self):\n",
    "      # Fake retriever\n",
    "      return self.documents\n",
    "  \n",
    "\n",
    "    def prepare_messages_for_llm(\n",
    "        self, messages: list[ChatAgentMessage]\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Filter out ChatAgentMessage fields that are not compatible with LLM message formats\"\"\"\n",
    "        compatible_keys = [\"role\", \"content\", \"name\", \"tool_calls\", \"tool_call_id\"]\n",
    "        return [\n",
    "            {\n",
    "                k: v\n",
    "                for k, v in m.model_dump_compat(exclude_none=True).items()\n",
    "                if k in compatible_keys\n",
    "            }\n",
    "            for m in messages\n",
    "        ]\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.PARSER)\n",
    "    def prepare_rag_prompt(self, messages):\n",
    "\n",
    "        docs = self.dummy_retriever()\n",
    "\n",
    "        messages = self.prepare_messages_for_llm(messages)\n",
    "\n",
    "        messages[-1]['content'] = f\"Answer the user's question based on the documents.\\nDocuments: <documents>{docs}</documents>.\\nUser's question: <user_question>{messages[-1]['content']}</user_question>\"\n",
    "\n",
    "        return messages\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        \n",
    "        messages = self.prepare_rag_prompt(messages)\n",
    "\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=self.llm_endpoint,\n",
    "            messages=messages,\n",
    "        )\n",
    "\n",
    "        return ChatAgentResponse(\n",
    "            messages=[\n",
    "                ChatAgentMessage(**resp.choices[0].message.to_dict(), id=resp.id)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        \n",
    "        messages = self.prepare_rag_prompt(messages)\n",
    "\n",
    "        for chunk in self.client.chat.completions.create(\n",
    "            model=self.llm_endpoint,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "        ):\n",
    "            if not chunk.choices or not chunk.choices[0].delta.content:\n",
    "                continue\n",
    "\n",
    "            yield ChatAgentChunk(\n",
    "                delta=ChatAgentMessage(\n",
    "                    **{\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": chunk.choices[0].delta.content,\n",
    "                        \"id\": chunk.id,\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "from mlflow.models import set_model\n",
    "\n",
    "AGENT = SimpleChatAgent()\n",
    "set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac4ad065-b40a-4853-a63e-912d0621bddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28a1de8-c080-4ab6-8a1a-eac6dd46d625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a10b819-6622-4fae-a084-faa36f51ab72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hello_world_agent import AGENT\n",
    "AGENT.predict({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"How do I monitor my genai app?\"}]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe3137b-e35a-4fad-99b6-c73ac5e81f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.resources import DatabricksServingEndpoint\n",
    "\n",
    "with mlflow.start_run():\n",
    "  model_info = mlflow.pyfunc.log_model(\n",
    "    python_model=\"hello_world_agent.py\",\n",
    "    artifact_path=\"agent\",\n",
    "    input_example={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"How do I monitor my genai app?\"}]\n",
    "    },  \n",
    "    resources=[DatabricksServingEndpoint(endpoint_name=\"databricks-meta-llama-3-3-70b-instruct\")],\n",
    "    pip_requirements=[\"databricks-sdk[openai]\", \"mlflow\", \"databricks-agents\", \"backoff\"],    \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c2522c-1aba-40ab-92f4-9feecb83eddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's validate that the model can be loaded, and try invoking it.\n",
    "mlflow.models.predict(\n",
    "    model_uri=model_info.model_uri,\n",
    "    input_data={\"messages\": [{\"role\": \"user\", \"content\": \"How do I monitor my genai app?\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c67e8bb-5b41-4d06-bdec-873561b6ea59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents import deploy\n",
    "import mlflow\n",
    "\n",
    "# Set the name of the model to use in your Unity Catalog schema defined at the top of this notebook\n",
    "\n",
    "MODEL_NAME = \"my_demo_agent\"\n",
    "\n",
    "# Register the model in Unity Catalog and deploy it as a serving endpoint\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "uc_model_info = mlflow.register_model(\n",
    "    model_uri=model_info.model_uri, name=f\"{UC_CATALOG}.{UC_SCHEMA}.{MODEL_NAME}\"\n",
    ")\n",
    "deployment = deploy(model_name=uc_model_info.name, model_version=uc_model_info.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c843dcb1-e0cb-4183-882f-c3acd506b0fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import EndpointStateReady, EndpointStateConfigUpdate\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import time\n",
    "\n",
    "print(\"\\nWaiting for endpoint to deploy.  This can take 10 - 20 minutes.\", end=\"\")\n",
    "w = WorkspaceClient()\n",
    "while w.serving_endpoints.get(deployment.endpoint_name).state.ready == EndpointStateReady.NOT_READY or w.serving_endpoints.get(deployment.endpoint_name).state.config_update == EndpointStateConfigUpdate.IN_PROGRESS:\n",
    "    print(\".\", end=\"\")\n",
    "    time.sleep(30)\n",
    "print(deployment.endpoint_name)\n",
    "print(\"\\nREADY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "856aa2ff-09ea-4cb2-a8b3-bf26ed8daaa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e869959d-0869-46dd-968e-325978abb0c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuring Quality Monitoring Metrics\n",
    "\n",
    "Since our agent was deployed using `agents.deploy`, basic monitoring (request volume, latency, errors) is already set up automatically.  The monitor is attached to this notebook's MLflow Experiment by default.\n",
    "\n",
    "Now we'll add quality evaluation metrics that use LLM judges to our monitoring. The monitoring configuration specified here will:\n",
    " - Sample 100% of requests for evaluation\n",
    " - Evaluate responses against safety, relevance, chunk relevance, groundedness (lack of hallucinations) and custom guidelines\n",
    "\n",
    "##### Agent Evaluation's built-in judges\n",
    "- Judges that run without ground-truth labels or retrieval in traces:\n",
    "  - `guideline_adherence`: guidelines allows developers write plain-language checklists or rubrics in their evaluation, improving transparency and trust with business stakeholders through easy-to-understand, structured grading rubrics. \n",
    "  - `safety`: making sure the response is safe\n",
    "  - `relevance_to_query`: making sure the response is relevant\n",
    "- For traces with retrieved docs (spans of type `RETRIEVER`):\n",
    "   - `groundedness`: detect hallucinations\n",
    "   - `chunk_relevance`: chunk-level relevance to the query\n",
    "\n",
    "See the full list of built-in judges ([AWS](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/llm-judge-reference) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/llm-judge-reference)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5d67a4-25de-4315-b930-7523b90cf5e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.evals.monitors import create_monitor, get_monitor, update_monitor, delete_monitor\n",
    "\n",
    "# Get the current monitor configuration \n",
    "monitor = get_monitor(endpoint_name=deployment.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "216cd05b-5979-4b64-b924-8d5569247a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update the monitor to add evaluation metrics\n",
    "monitor = update_monitor(\n",
    "    endpoint_name=deployment.endpoint_name,\n",
    "    monitoring_config={\n",
    "        \"sample\": 1,  # Sample 100% of requests - this can be any number from 0 (0%) to 1 (100%).\n",
    "        # Select 0+ of Agent Evaluation's built-in judges\n",
    "        \"metrics\": ['guideline_adherence', 'groundedness', 'safety', 'relevance_to_query', 'chunk_relevance'],\n",
    "        # Customize these guidelines based on your business requirements.  These guidelines will be analyzed using Agent Evaluation's built in guideline_adherence judge\n",
    "        \"global_guidelines\": {\n",
    "            \"english\": [\"The response must be in English.\"],\n",
    "            \"clarity\": [\"The response must be clear, coherent, and concise.\"],\n",
    "            \"relevant_if_not_refusal\": [\"Determine if the response provides an answer to the user's request.  A refusal to answer is considered relevant.  However, if the response is NOT a refusal BUT also doesn't provide relevant information, then the answer is not relevant.\"],\n",
    "            \"no_answer_if_no_docs\": [\"If the agent can not find a relevant document, it should refuse to answer the question and not discuss the reasons why it could not answer.\"]\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd876475-1149-497a-ba36-e03972b75363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Sample Traffic\n",
    "\n",
    "Now that our endpoint is deployed, we'll send some sample questions to generate traffic for monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4337b75e-2770-4699-aada-a7489368bec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Helper function to send simulated traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c18478cd-d088-4f90-a6be-01aa98878348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import deployments\n",
    "\n",
    "client = deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "questions = [\n",
    "    \"What is Mosaic AI Agent Evaluation?\",\n",
    "    \"How do you use MLflow with Databricks for experiment tracking?\",\n",
    "    \"What should I use Databricks Feature Store for?\",\n",
    "    \"How does AutoML work in Databricks?\",\n",
    "    \"What is Model Serving in Databricks and what are its deployment options?\",\n",
    "    \"How does Databricks handle distributed deep learning training?\",\n",
    "    \"Does Unity Catalog support models?\",\n",
    "    \"What is the Databricks Lakehouse?\",\n",
    "    \"Which Llama models are supported on Databricks?\",\n",
    "    \"How does Databricks integrate with popular ML frameworks like PyTorch and TensorFlow?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\nQuestion {i}: {question}\")  \n",
    "    response = client.predict(\n",
    "        endpoint=deployment.endpoint_name,\n",
    "        inputs={\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    print(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52c0c711-f2a0-4779-8690-1a49d93a04ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## [Optional] Enable integration with Review App and Evaluation Sets\n",
    "\n",
    "To fix any quality issues identified in the monitoring dashboard, you can:\n",
    "1. Copy the production trace to an evaluation dataset to use it as a test case in `mlflow.evaluate(...)`\n",
    "2. Send the production trace to the Review App to collect domain expert input/labels\n",
    "\n",
    "To enable these features, you need to create an Evaluation Set and Labeling Session.  For more information on these concepts, see [the documentation](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/review-app#datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "368117da-e361-48cf-a0f9-6921aabe5d86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. Create an evaluation dataset\n",
    "\n",
    "Your monitor will show all evaluation datasets linked to the MLflow Eperiment where the monitor is configured - by default, this is the Notebook's MLflow Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd69edf2-d256-4e6b-a2dd-5943f2dfb486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents import datasets\n",
    "from databricks.sdk.errors.platform import NotFound\n",
    "\n",
    "# Make sure you have updated the uc_catalog & uc_schema widgets to a valid catalog/schema where you have CREATE TABLE permissions.\n",
    "EVAL_DATASET_NAME = \"agent_evaluation_set\"\n",
    "\n",
    "UC_TABLE_NAME = f'{UC_CATALOG}.{UC_SCHEMA}.{EVAL_DATASET_NAME}'\n",
    "\n",
    "# Remove the evaluation dataset if it already exists\n",
    "try:\n",
    "  datasets.delete_dataset(UC_TABLE_NAME)\n",
    "except NotFound:\n",
    "  pass\n",
    "\n",
    "# Create the evaluation dataset\n",
    "dataset = datasets.create_dataset(UC_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09aa5752-8a89-4ef9-996e-062b98f9ee14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. Create a labeling session\n",
    "\n",
    "Your monitor will show all labeling sessions linked to the MLflow Eperiment where the monitor is configured - by default, this is the Notebook's MLflow Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3383b8c-c141-4441-95e1-efed4857ea87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents import review_app\n",
    "\n",
    "# OPTIONAL: Add a comma separated list of domain experts who will provide feedback/alebsl\n",
    "# If not provided, only the user running this notebook will be granted access to the review app.\n",
    "DOMAIN_EXPERT_EMAILS = []\n",
    "\n",
    "# Get the Review App from the current MLflow Experiment\n",
    "my_review_app = review_app.get_review_app()\n",
    "\n",
    "# Optional: Add a custom question for your domain experts\n",
    "my_review_app.create_label_schema(\n",
    "  name=\"good_response\",\n",
    "  # Type can be \"expectation\" or \"feedback\".\n",
    "  type=\"feedback\",\n",
    "  title=\"Is this a good response?\",\n",
    "  input=review_app.label_schemas.InputCategorical(options=[\"Yes\", \"No\"]),\n",
    "  instruction=\"Optional: provide a rationale below.\",\n",
    "  enable_comment=True,\n",
    "  overwrite=True\n",
    ")\n",
    "\n",
    "my_session = my_review_app.create_labeling_session(\n",
    "    name=\"collect_facts\",\n",
    "    assigned_users=DOMAIN_EXPERT_EMAILS, # If not provided, only the user running this notebook will be granted access\n",
    "    # Built-in labeling schemas: EXPECTED_FACTS, GUIDELINES, EXPECTED_RESPONSE\n",
    "    label_schemas=[review_app.label_schemas.GUIDELINES,  \"good_response\"],\n",
    ")\n",
    "\n",
    "# URLs to share with the SME.\n",
    "print(\"Review App URL:\", my_review_app.url)\n",
    "print(\"Labeling session URL: \", my_session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff868448-4838-498e-990c-9d0865e2f04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Viewing Monitoring Results\n",
    "\n",
    "The monitoring results are stored in Delta tables and can be accessed in two ways:\n",
    "1. Through the MLflow UI (click the link generated above)\n",
    "1. Directly querying the Delta table containing evaluated traces\n",
    "\n",
    "Below, we'll query the Delta table to see the evaluation results, filtering out skipped evaluations.\n",
    "\n",
    "If you do not see monitoring results, wait until the next run of the monitoring job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f76e44-bc8b-44d1-aef1-5aa983cfbdb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read evaluated traces from Delta\n",
    "display(spark.table(monitor.evaluated_traces_table).filter(\"evaluation_status != 'skipped'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83c2d18c-cd59-4ac3-b3e9-fd7935cf2414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "When you're done with the demo, you can delete the endpoint and the monitor using the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d58c223b-7f6d-49c6-9f6f-2b8748a89828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "databricks.agents.delete_deployment(model_name=uc_model_info.name, model_version=uc_model_info.version)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "agent-monitoring-example-no-feedback",
   "widgets": {
    "uc_catalog": {
     "currentValue": "benmackenzie_catalog",
     "nuid": "ef723929-9c21-4383-b458-1a9e8004d262",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbacademy",
      "label": null,
      "name": "uc_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbacademy",
      "label": null,
      "name": "uc_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "uc_schema": {
     "currentValue": "playground",
     "nuid": "fce6723f-5a83-4e65-9702-e889840f079e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "uc_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": null,
      "name": "uc_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
